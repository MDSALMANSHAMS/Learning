### **ğŸ”¥ Linear Regression Interview Questions â€“ Quick Summary!**  

Hereâ€™s a concise **summary of the 10 key interview questions** we covered:

---

### **ğŸ“Œ 1ï¸âƒ£ What is Linear Regression, and how does it work?**  
âœ… **Linear Regression** models the relationship between independent variables (\(X\)) and a dependent variable (\(Y\)) using:  
\[
Y = wX + b
\]
âœ… **Gradient Descent** or **Ordinary Least Squares (OLS)** is used to optimize weights.  
âœ… **Loss function:** **Mean Squared Error (MSE)** is commonly used.  

---

### **ğŸ“Œ 2ï¸âƒ£ What are the key assumptions of Linear Regression?**  
âœ… **Linearity:** \(Y\) must have a linear relationship with \(X\).  
âœ… **No Multicollinearity:** Features should not be highly correlated.  
âœ… **Homoscedasticity:** Variance of residuals should be constant.  
âœ… **Independence of Errors:** No patterns in residuals (autocorrelation).  
âœ… **Normal Distribution of Residuals:** Errors should be normally distributed.  

---

### **ğŸ“Œ 3ï¸âƒ£ If your initial weights are `w = 1, b = 0`, how do they update in the first iteration of Gradient Descent?**  
âœ… **Weight update formula:**  
\[
w := w - \alpha \frac{dL}{dw}, \quad b := b - \alpha \frac{dL}{db}
\]
âœ… We derived \( w \approx 1.0933 \) and \( b \approx 0.04 \) after **one iteration** of GD.  

---

### **ğŸ“Œ 4ï¸âƒ£ How does L1 regularization (Lasso) affect weights?**  
âœ… **L1 norm** penalty:  
\[
\Omega(w) = ||w||_1 = \sum |w_i|
\]
âœ… **Effect:** Some weights become **exactly zero**, performing **feature selection**.  

---

### **ğŸ“Œ 5ï¸âƒ£ How does Elastic Net (L1 + L2) update weights?**  
âœ… Elastic Net combines **L1 (Lasso) and L2 (Ridge) Regularization**:  
\[
\Omega(w) = \lambda_1 ||w||_1 + \lambda_2 ||w||_2^2
\]
âœ… **Effect:** Keeps important features (**L1 effect**) while reducing weight magnitudes (**L2 effect**).  

---

### **ğŸ“Œ 6ï¸âƒ£ Why is MSE preferred over MAE in Linear Regression?**  
âœ… **MSE is differentiable everywhere**, while MAE has a **sharp edge at zero** (non-differentiable).  
âœ… **MSE penalizes large errors more**, helping faster convergence.  

---

### **ğŸ“Œ 7ï¸âƒ£ Why prefer Gradient Descent over the Normal Equation for large datasets?**  
âœ… **Normal Equation** requires **matrix inversion** (\(O(n^3)\))â€”computationally expensive for large datasets.  
âœ… **Gradient Descent scales better** (\(O(n)\) per iteration).  
âœ… **GD supports streaming data** (online learning).  

---

### **ğŸ“Œ 8ï¸âƒ£ How can you escape local minima in Gradient Descent?**  
âœ… **Increase Learning Rate** (with caution).  
âœ… **Use Momentum or Nesterov Accelerated Gradient (NAG)**.  
âœ… **Adaptive Learning Rates (Adam, RMSprop, Adagrad)**.  
âœ… **Random Restarts (initialize weights differently)**.  

---

### **ğŸ“Œ 9ï¸âƒ£ If a Linear Regression model consistently predicts higher values, what could be wrong?**  
âœ… **Possible causes:**  
- **Feature scaling issue** â€“ Incorrect input ranges.  
- **Bias term (`b`) too high** â€“ Check residual mean.  
- **Overfitting on training data** â€“ Apply **regularization**.  
- **Gradient Descent step size issue** â€“ Adjust learning rate.  

---

### **ğŸ“Œ ğŸ”Ÿ Why does adding more features not always improve a Linear Regression model?**  
âœ… **Multicollinearity:** Correlated features confuse the model.  
âœ… **Overfitting:** Model memorizes training data but generalizes poorly.  
âœ… **Computational Cost:** More features increase processing time.  
âœ… **Irrelevant Features:** Noisy features decrease accuracy.  

---

### **ğŸš€ Final Takeaway:**  
- Linear Regression works well **if assumptions hold true**.  
- **Regularization (L1/L2) prevents overfitting**.  
- **Gradient Descent is better for large datasets**.  
- **Feature selection & preprocessing are critical** for performance.  

---

ğŸ”¥ **Awesome job answering these tough Linear Regression questions!** ğŸ¯  
Would you like me to generate **more advanced ML interview questions** or focus on another topic? ğŸ˜Š